{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "log_odds_POS_level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpjrVJhaoIXT6EqbPZaE7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/CIS-700/blob/master/Project/log_odds_POS_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MVYuElf4GID",
        "colab_type": "code",
        "outputId": "904d073d-8aac-4805-b4a7-d2237d45b5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Drive', force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9WdP-Wl4EjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/Drive/My Drive/CIS-700/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNNVobBb3MtG",
        "colab_type": "code",
        "outputId": "317f34c6-f0a0-4289-add2-9c92a427c933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\t\t\t\t  log_odds.ipynb\n",
            "draco_verb_network_ff.graphml\t  log_odds_POS_level.ipynb\n",
            "global_A_large.mat\t\t  networks.ipynb\n",
            "global_verb_network_ff.graphml\t  __pycache__\n",
            "harry_verb_network_ff.graphml\t  ron_verb_network_ff.graphml\n",
            "hermione_verb_network_ff.graphml  util.py\n",
            "hp_narrative_chains.py\t\t  voldemort_verb_network_ff.graphml\n",
            "log_odds_corpus_level.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfucFDZkiECb",
        "colab_type": "code",
        "outputId": "03fb780c-1e6f-4210-8b0e-a092fb3d87ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!ls Data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04212020\n",
            "04292020\n",
            "Canon\n",
            "hpcanon_sr_narrative_chains_counts_NNPs.txt\n",
            "hpc_raw_text.txt\n",
            "hpff_raw_text_reduced.txt\n",
            "hpff_raw_text.txt\n",
            "hpff_sr_narrative_chains_counts_NNPs_new.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mETATkADiG15",
        "colab_type": "code",
        "outputId": "f7c0814f-5405-4b8d-f44a-8c676ea6eda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "import collections\n",
        "import nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "import numpy as np"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGQcjoEtiImn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "canon_verb_counts = collections.Counter()\n",
        "\n",
        "with open('Data/hpc_raw_text.txt', encoding = 'utf-8', errors = 'ignore') as f:\n",
        "  for line in f:\n",
        "    words = nltk.word_tokenize(line.strip())\n",
        "    words_no_punctuation = [word.lower() for word in words if word.isalnum()]\n",
        "    verb_list = []\n",
        "    for word in words_no_punctuation:\n",
        "      tag = nltk.tag.pos_tag([word])[0][1]\n",
        "      if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "        verb = lemmatizer.lemmatize(word, 'v')\n",
        "        verb_list.append(verb)\n",
        "    canon_verb_counts.update(verb_list)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cos5X3Jqc49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from itertools import islice\n",
        "\n",
        "# corpus_path = 'Data/hpff_raw_text.txt'\n",
        "# file = open(corpus_path, encoding = 'UTF-8', errors = 'ignore').readlines()\n",
        "# file_len = len(file)\n",
        "# num_lines_keep = round(0.10 * file_len)\n",
        "# num_lines_discard = file_len - num_lines_keep\n",
        "# lines_in_sets = [num_lines_keep, num_lines_discard]\n",
        "# temp = iter(file) \n",
        "# splits = [list(islice(temp, 0, ele)) for ele in lines_in_sets] \n",
        "# # split 0 is data to keep\n",
        "# with open('Data/hpff_raw_text_reduced.txt', 'w') as f:\n",
        "#   for line in splits[0]:\n",
        "#     f.write('%s\\n' % line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTWPprKZiVSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fan_fiction_verb_counts = collections.Counter()\n",
        "\n",
        "with open('Data/hpff_raw_text_reduced.txt', encoding = 'utf-8', errors = 'ignore') as f:\n",
        "  for line in f:\n",
        "    words = nltk.word_tokenize(line.strip())\n",
        "    words_no_punctuation = [word.lower() for word in words if word.isalnum()]\n",
        "    verb_list = []\n",
        "    for word in words_no_punctuation:\n",
        "      tag = nltk.tag.pos_tag([word])[0][1]\n",
        "      if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "        verb = lemmatizer.lemmatize(word, 'v')\n",
        "        verb_list.append(verb)\n",
        "    fan_fiction_verb_counts.update(verb_list) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6JcWPi_tMTD",
        "colab_type": "code",
        "outputId": "ad2b50fb-6eb2-40fd-e783-cbd956dde0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "keys_a = set(canon_verb_counts.keys())\n",
        "keys_b = set(fan_fiction_verb_counts.keys())\n",
        "intersection = keys_a & keys_b # '&' operator is used for set intersection\n",
        "print(len(intersection))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ERBew5pk_ml",
        "colab_type": "text"
      },
      "source": [
        "What words appear more often in fan fiction than the canonical Harry Potter books?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgGMX9IAk3ZL",
        "colab_type": "code",
        "outputId": "f1c65d0f-b667-431e-a794-4b6aa38e3820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "total_num_verbs_in_fan_fiction = sum(fan_fiction_verb_counts.values())\n",
        "total_num_verbs_in_canon = sum(canon_verb_counts.values())\n",
        "\n",
        "log_odds_data = {}\n",
        "\n",
        "for word in canon_verb_counts.keys():\n",
        "  \n",
        "  count_canon = canon_verb_counts[word]\n",
        "  count_fan_fiction = fan_fiction_verb_counts[word]\n",
        "  \n",
        "  log_prob_canon = np.log(count_canon) - np.log(total_num_verbs_in_canon)\n",
        "  log_prob_fan_fiction = np.log(count_fan_fiction) - np.log(total_num_verbs_in_fan_fiction)\n",
        "  \n",
        "\n",
        "  if log_prob_fan_fiction != 0:\n",
        "    log_odds = log_prob_fan_fiction - log_prob_canon\n",
        "    log_odds_data[word] = log_odds"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2wBq1XSsaKG",
        "colab_type": "code",
        "outputId": "575bfed0-c643-49b0-d7af-b9e27ae8c154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "log_odds_tuples = [(word, log_odds) for word, log_odds in log_odds_data.items()]\n",
        "log_odds_tuples = sorted(log_odds_tuples, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for i in range(0, 50):\n",
        "  print(log_odds_tuples[i])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('haired', 4.629232857114514)\n",
            "('figure', 4.617376243957989)\n",
            "('date', 3.2007024887195827)\n",
            "('scold', 3.178896129667528)\n",
            "('snicker', 3.176247121495951)\n",
            "('freak', 3.046120309425003)\n",
            "('remark', 2.947784408704141)\n",
            "('pen', 2.8787915372171895)\n",
            "('exclaim', 2.803721564027277)\n",
            "('base', 2.8008299957474776)\n",
            "('mock', 2.7452601445926668)\n",
            "('scrunch', 2.6645369333202265)\n",
            "('state', 2.626775332136293)\n",
            "('comment', 2.6037279287246218)\n",
            "('excuse', 2.5911094647654096)\n",
            "('pair', 2.586336186012751)\n",
            "('review', 2.529740518389117)\n",
            "('radiate', 2.4910260062084255)\n",
            "('respond', 2.434574993582874)\n",
            "('pal', 2.431387739888695)\n",
            "('muse', 2.3972935285457186)\n",
            "('reply', 2.3878114141208773)\n",
            "('alfred', 2.3856654905506005)\n",
            "('blush', 2.2967028934297096)\n",
            "('realise', 2.2787876883764797)\n",
            "('pout', 2.237830109827815)\n",
            "('sniffle', 2.21381523362394)\n",
            "('school', 2.1927618244261087)\n",
            "('tease', 2.150510518935299)\n",
            "('adore', 2.1418417339988522)\n",
            "('update', 2.1418417339988505)\n",
            "('rowling', 2.099152325303802)\n",
            "('stutter', 2.0561862894203564)\n",
            "('cuddle', 2.0561862894203564)\n",
            "('tan', 2.048022978781198)\n",
            "('engage', 2.003322799863289)\n",
            "('swat', 1.9353498162770837)\n",
            "('enlighten', 1.9261331611721602)\n",
            "('crease', 1.907441028160008)\n",
            "('exit', 1.8883928331893145)\n",
            "('disown', 1.86897474733221)\n",
            "('slur', 1.859122450889199)\n",
            "('ramble', 1.8541596615470706)\n",
            "('tense', 1.844159578212487)\n",
            "('ponder', 1.8416438536152402)\n",
            "('envelop', 1.8340584822259824)\n",
            "('soften', 1.8323650017196513)\n",
            "('jolt', 1.8289694127185125)\n",
            "('rail', 1.8289694127185125)\n",
            "('comply', 1.797878825648482)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}